# -*- coding: utf-8 -*-
"""SLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j-KBZAJ3duB1HRNYAuc-dGeye_Nx8ZLH
"""

#Install required libraries
!pip install transformers torch sentencepiece PyPDF2 sentence-transformers faiss-cpu gradio

#Importing necessary req. libraries
import PyPDF2
import faiss
import gradio as gr
import torch
from sentence_transformers import SentenceTransformer
from transformers import pipeline

#Extract Text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() + "n"
    return text

#Split Text into Chunks (for the speed)
def split_text_into_chunks(text, chunk_size=300):
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

#Convert Text to Embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

#Build FAISS Index
def build_faiss_index(chunk_embeddings):
    d = chunk_embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(chunk_embeddings)
    return index

#Retrieve Relevant Chunks (Set top_k=1 for speed)
def get_relevant_chunks(question, index, chunks, top_k=1):
    question_embedding = embedding_model.encode([question])
    distances, indices = index.search(question_embedding, top_k)
    relevant_chunks = [chunks[idx] for idx in indices[0]]
    return relevant_chunks

#Load Fast Question answering Model
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2", device=0 if torch.cuda.is_available() else -1)  # Uses GPU if available

#Generating Answer
def generate_response(question, relevant_chunks):
    context = " ".join(relevant_chunks)
    response = qa_pipeline(question=question, context=context)
    return response["answer"]

#Main Chatbot
def chatbot(question):
    relevant_chunks = get_relevant_chunks(question, index, chunks)
    response = generate_response(question, relevant_chunks)
    return response

#PDF and Prepare Data
pdf_path = "/content/jess401.pdf"
book_text = extract_text_from_pdf(pdf_path)
chunks = split_text_into_chunks(book_text)
chunk_embeddings = embedding_model.encode(chunks)
index = build_faiss_index(chunk_embeddings)

#Launching the Gradio Chatbot
iface = gr.Interface(fn=chatbot, inputs="text", outputs="text", title="PDF-Based Question Answering Chatbot")
iface.launch(share=True)